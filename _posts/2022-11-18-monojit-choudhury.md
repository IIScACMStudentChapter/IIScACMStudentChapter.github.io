---
layout: post
title: "Invited talk by Dr. Monojit Choudhury, Turing Lab, MSR, India"
date: 2022-11-18
tags: [event, talks]
comment: false
summary: The Chapter will host an Invited talk by Dr. Monojit Choudhury, Microsoft Research India
---

<section class="blog-area bg-color1">
      <div class="blog-wrapper bminus-30">
         <div class="container">
            <div class="row">
               <div class="col-md-12 col-lg-12">
                  <div class="single-post">
                    <div class="container">
    <div class="row">
        <div class="col-lg-9" style="margin: auto">
                <div class="post-carousel owl-carousel">
                            <div class="slider-content wow fadeIn" data-wow-duration="3s">
                                <img src="/assets/monojit-choudhury/1.jpg" alt="image" />                             
                            </div>
                            <div class="slider-content wow fadeIn" data-wow-duration="3s">
                                <img src="/assets/monojit-choudhury/2.jpg" alt="image" />                             
                            </div>
                </div>
        </div>
    </div>
</div>
   
   <div class="single-content">
      <p><br /></p>

<p>Invited talk by <b>Dr. Monojit Choudhury </b>, Microsoft Research Lab, India </p>


<h6>Title</h6>

<p> Predicting and Explaining Cross-lingual Zero-shot and Few-shot Transfer</p>

<h6>Abstract</h6>

<p>
Given a massively multilingual language models (MMLM), can we predict the accuracy of cross-lingual zero-shot and few-shot transfer for a task on target languages with little or no test data? This seemingly impossible task, if solved, can have several potential benefits. First, we could estimate the performance of a model even in languages where a test set is not available, and/or building one is difficult. Second, one can predict training data configurations that would give certain desired performance across a set of languages, and accordingly strategize data collection plans; this in turn can lead to linguistically fair MMLM-based models. Third, as a by-product, we would know which factors influence cross-lingual transfer. In this talk, I will give an overview of Project LITMUS â€“ Linguistically Informed Training and Testing of Multilingual Systems, where we build several ML models for performance prediction; besides their applications, I will discuss what we learn about the factors that influence cross-lingual transfer. 
</p>

<h6>Biography</h6>
<p>
https://www.microsoft.com/en-us/research/people/monojitc/
</p>

<h6>Date and Time</h6>

<p>18-Nov-2022 (Friday) 3PM to 4PM</p>

<h6>Venue</h6>

<p>
CDS Seminar Hall, Room No. 102
</p>

<p>
All are welcome. High Tea will be served at 4:00 PM.
</p>
